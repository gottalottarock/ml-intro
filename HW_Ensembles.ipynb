{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным и сравнить его эффективность с ансамблями из самых популярных библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    make_scorer,\n",
    "    accuracy_score,\n",
    "    RocCurveDisplay,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from scipy.stats import mode\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=\"auto\",\n",
    "        n_estimators=10,\n",
    "        save_samples_idx=False,\n",
    "    ):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self._trees = None\n",
    "        self._features = None\n",
    "        self._save_samples_idx = save_samples_idx\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        if self.max_features == \"auto\":\n",
    "            self.max_features_n = int(np.sqrt(n))\n",
    "        self._trees = []\n",
    "        self._features = []\n",
    "        self._samples_idx = []\n",
    "        all_features = np.arange(n)\n",
    "        all_samples = np.arange(m)\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeClassifier(\n",
    "                criterion=self.criterion,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "            )\n",
    "            tree_features = np.random.choice(\n",
    "                all_features, size=self.max_features_n, replace=False\n",
    "            )\n",
    "            samples_idx = np.random.choice(all_samples, size=m, replace=True)\n",
    "            if self._save_samples_idx:\n",
    "                self._samples_idx.append(samples_idx)\n",
    "            X_tree = X[samples_idx][:, tree_features]\n",
    "            y_tree = y[samples_idx]\n",
    "            tree.fit(X_tree, y_tree)\n",
    "            self._trees.append(tree)\n",
    "            self._features.append(tree_features)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # only binary\n",
    "        predictions = []\n",
    "        for tree, features in zip(self._trees, self._features):\n",
    "            X_tree = X[:, features]\n",
    "            preds = tree.predict(X_tree)\n",
    "            predictions.append(preds)\n",
    "        predictions = np.stack(predictions)\n",
    "        return predictions.sum(axis=0) / predictions.shape[0]\n",
    "\n",
    "    def predict_nonbinary(self, X):\n",
    "        predictions = []\n",
    "        for tree, features in zip(self._trees, self._features):\n",
    "            X_tree = X[:, features]\n",
    "            preds = tree.predict(X_tree)\n",
    "            predictions.append(preds)\n",
    "        predictions = np.stack(predictions)\n",
    "        return mode(predictions, axis=0)[0]\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"criterion\": self.criterion,\n",
    "            \"max_depth\": self.max_depth,\n",
    "            \"min_samples_leaf\": self.min_samples_leaf,\n",
    "            \"max_features\": self.max_features,\n",
    "            \"n_estimators\": self.n_estimators,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        return self.__class__(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./homework-ensembles-ib-22/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_dataset(DATA_PATH=DATA_PATH):\n",
    "    df_train = pd.read_csv(Path(DATA_PATH) / \"x_spam_train.csv\")\n",
    "    df_train = df_train.drop(columns=[\"Id\"])\n",
    "    df_test = pd.read_csv(Path(DATA_PATH) / \"y_spam_train.csv\")\n",
    "    df_test = df_test.drop(columns=[\"Id\"])\n",
    "    return df_train.values, df_test.values.reshape(df_train.values.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(DATA_PATH=DATA_PATH):\n",
    "    df_train = pd.read_csv(Path(DATA_PATH) / \"x_spam_test.csv\")\n",
    "    df_train = df_train.drop(columns=[\"Id\"])\n",
    "    return df_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_train_dataset()\n",
    "X_test = load_test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943739310468991"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_classifier = RandomForestClassifier()\n",
    "test_classifier.fit(X_train, y_train)\n",
    "preds = test_classifier.predict(X_train)\n",
    "roc_auc_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "видно, что переобучение есть, значит как-то работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2 (2 балла)\n",
    "Оптимизируйте по `AUC` на кроссвалидации (размер валидационной выборки - 20%) параметры своей реализации `Random Forest`: \n",
    "\n",
    "максимальную глубину деревьев из [2, 3, 5, 7, 10], количество деревьев из [5, 10, 20, 30, 50, 100]. \n",
    "\n",
    "Постройте `ROC` кривую (и выведите `AUC` и `accuracy`) для лучшего варианта.\n",
    "\n",
    "Подсказка: можно построить сразу 100 деревьев глубины 10, а потом убирать деревья и\n",
    "глубину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\": [2, 3, 5, 7, 10], \"n_estimators\": [5, 10, 20, 30, 50, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=param_gird,\n",
    "    scoring=roc_auc_scorer,\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV 1/5] END .......max_depth=2, n_estimators=5;, score=0.867 total time=   0.0s\n",
      "[CV 2/5] END .......max_depth=2, n_estimators=5;, score=0.922 total time=   0.0s\n",
      "[CV 3/5] END .......max_depth=2, n_estimators=5;, score=0.920 total time=   0.0s\n",
      "[CV 4/5] END .......max_depth=2, n_estimators=5;, score=0.914 total time=   0.0s\n",
      "[CV 5/5] END .......max_depth=2, n_estimators=5;, score=0.925 total time=   0.0s\n",
      "[CV 1/5] END ......max_depth=2, n_estimators=10;, score=0.903 total time=   0.0s\n",
      "[CV 2/5] END ......max_depth=2, n_estimators=10;, score=0.917 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=2, n_estimators=10;, score=0.912 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=2, n_estimators=10;, score=0.933 total time=   0.0s\n",
      "[CV 5/5] END ......max_depth=2, n_estimators=10;, score=0.905 total time=   0.0s\n",
      "[CV 1/5] END ......max_depth=2, n_estimators=20;, score=0.946 total time=   0.1s\n",
      "[CV 2/5] END ......max_depth=2, n_estimators=20;, score=0.940 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=2, n_estimators=20;, score=0.943 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=2, n_estimators=20;, score=0.940 total time=   0.1s\n",
      "[CV 5/5] END ......max_depth=2, n_estimators=20;, score=0.947 total time=   0.1s\n",
      "[CV 1/5] END ......max_depth=2, n_estimators=30;, score=0.946 total time=   0.1s\n",
      "[CV 2/5] END ......max_depth=2, n_estimators=30;, score=0.937 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=2, n_estimators=30;, score=0.938 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=2, n_estimators=30;, score=0.951 total time=   0.1s\n",
      "[CV 5/5] END ......max_depth=2, n_estimators=30;, score=0.946 total time=   0.1s\n",
      "[CV 1/5] END ......max_depth=2, n_estimators=50;, score=0.945 total time=   0.2s\n",
      "[CV 2/5] END ......max_depth=2, n_estimators=50;, score=0.950 total time=   0.2s\n",
      "[CV 3/5] END ......max_depth=2, n_estimators=50;, score=0.946 total time=   0.2s\n",
      "[CV 4/5] END ......max_depth=2, n_estimators=50;, score=0.956 total time=   0.2s\n",
      "[CV 5/5] END ......max_depth=2, n_estimators=50;, score=0.953 total time=   0.3s\n",
      "[CV 1/5] END .....max_depth=2, n_estimators=100;, score=0.941 total time=   0.3s\n",
      "[CV 2/5] END .....max_depth=2, n_estimators=100;, score=0.951 total time=   0.4s\n",
      "[CV 3/5] END .....max_depth=2, n_estimators=100;, score=0.949 total time=   0.4s\n",
      "[CV 4/5] END .....max_depth=2, n_estimators=100;, score=0.956 total time=   0.4s\n",
      "[CV 5/5] END .....max_depth=2, n_estimators=100;, score=0.950 total time=   0.3s\n",
      "[CV 1/5] END .......max_depth=3, n_estimators=5;, score=0.867 total time=   0.0s\n",
      "[CV 2/5] END .......max_depth=3, n_estimators=5;, score=0.910 total time=   0.0s\n",
      "[CV 3/5] END .......max_depth=3, n_estimators=5;, score=0.901 total time=   0.0s\n",
      "[CV 4/5] END .......max_depth=3, n_estimators=5;, score=0.898 total time=   0.0s\n",
      "[CV 5/5] END .......max_depth=3, n_estimators=5;, score=0.909 total time=   0.0s\n",
      "[CV 1/5] END ......max_depth=3, n_estimators=10;, score=0.937 total time=   0.0s\n",
      "[CV 2/5] END ......max_depth=3, n_estimators=10;, score=0.892 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=3, n_estimators=10;, score=0.926 total time=   0.0s\n",
      "[CV 4/5] END ......max_depth=3, n_estimators=10;, score=0.905 total time=   0.0s\n",
      "[CV 5/5] END ......max_depth=3, n_estimators=10;, score=0.954 total time=   0.0s\n",
      "[CV 1/5] END ......max_depth=3, n_estimators=20;, score=0.922 total time=   0.1s\n",
      "[CV 2/5] END ......max_depth=3, n_estimators=20;, score=0.949 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=3, n_estimators=20;, score=0.944 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=3, n_estimators=20;, score=0.939 total time=   0.1s\n",
      "[CV 5/5] END ......max_depth=3, n_estimators=20;, score=0.940 total time=   0.1s\n",
      "[CV 1/5] END ......max_depth=3, n_estimators=30;, score=0.936 total time=   0.2s\n",
      "[CV 2/5] END ......max_depth=3, n_estimators=30;, score=0.950 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=3, n_estimators=30;, score=0.945 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=3, n_estimators=30;, score=0.957 total time=   0.1s\n",
      "[CV 5/5] END ......max_depth=3, n_estimators=30;, score=0.954 total time=   0.1s\n",
      "[CV 1/5] END ......max_depth=3, n_estimators=50;, score=0.948 total time=   0.2s\n",
      "[CV 2/5] END ......max_depth=3, n_estimators=50;, score=0.962 total time=   0.2s\n",
      "[CV 3/5] END ......max_depth=3, n_estimators=50;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END ......max_depth=3, n_estimators=50;, score=0.965 total time=   0.2s\n",
      "[CV 5/5] END ......max_depth=3, n_estimators=50;, score=0.949 total time=   0.2s\n",
      "[CV 1/5] END .....max_depth=3, n_estimators=100;, score=0.948 total time=   0.5s\n",
      "[CV 2/5] END .....max_depth=3, n_estimators=100;, score=0.957 total time=   0.4s\n",
      "[CV 3/5] END .....max_depth=3, n_estimators=100;, score=0.948 total time=   0.4s\n",
      "[CV 4/5] END .....max_depth=3, n_estimators=100;, score=0.960 total time=   0.4s\n",
      "[CV 5/5] END .....max_depth=3, n_estimators=100;, score=0.954 total time=   0.5s\n",
      "[CV 1/5] END .......max_depth=5, n_estimators=5;, score=0.928 total time=   0.0s\n",
      "[CV 2/5] END .......max_depth=5, n_estimators=5;, score=0.926 total time=   0.0s\n",
      "[CV 3/5] END .......max_depth=5, n_estimators=5;, score=0.913 total time=   0.0s\n",
      "[CV 4/5] END .......max_depth=5, n_estimators=5;, score=0.951 total time=   0.0s\n",
      "[CV 5/5] END .......max_depth=5, n_estimators=5;, score=0.941 total time=   0.0s\n",
      "[CV 1/5] END ......max_depth=5, n_estimators=10;, score=0.934 total time=   0.0s\n",
      "[CV 2/5] END ......max_depth=5, n_estimators=10;, score=0.948 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=5, n_estimators=10;, score=0.926 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=5, n_estimators=10;, score=0.916 total time=   0.0s\n",
      "[CV 5/5] END ......max_depth=5, n_estimators=10;, score=0.955 total time=   0.1s\n",
      "[CV 1/5] END ......max_depth=5, n_estimators=20;, score=0.958 total time=   0.1s\n",
      "[CV 2/5] END ......max_depth=5, n_estimators=20;, score=0.934 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=5, n_estimators=20;, score=0.949 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=5, n_estimators=20;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END ......max_depth=5, n_estimators=20;, score=0.964 total time=   0.1s\n",
      "[CV 1/5] END ......max_depth=5, n_estimators=30;, score=0.962 total time=   0.2s\n",
      "[CV 2/5] END ......max_depth=5, n_estimators=30;, score=0.964 total time=   0.2s\n",
      "[CV 3/5] END ......max_depth=5, n_estimators=30;, score=0.957 total time=   0.2s\n",
      "[CV 4/5] END ......max_depth=5, n_estimators=30;, score=0.963 total time=   0.2s\n",
      "[CV 5/5] END ......max_depth=5, n_estimators=30;, score=0.961 total time=   0.2s\n",
      "[CV 1/5] END ......max_depth=5, n_estimators=50;, score=0.956 total time=   0.2s\n",
      "[CV 2/5] END ......max_depth=5, n_estimators=50;, score=0.964 total time=   0.3s\n",
      "[CV 3/5] END ......max_depth=5, n_estimators=50;, score=0.958 total time=   0.3s\n",
      "[CV 4/5] END ......max_depth=5, n_estimators=50;, score=0.955 total time=   0.2s\n",
      "[CV 5/5] END ......max_depth=5, n_estimators=50;, score=0.955 total time=   0.2s\n",
      "[CV 1/5] END .....max_depth=5, n_estimators=100;, score=0.964 total time=   0.6s\n",
      "[CV 2/5] END .....max_depth=5, n_estimators=100;, score=0.970 total time=   0.5s\n",
      "[CV 3/5] END .....max_depth=5, n_estimators=100;, score=0.957 total time=   0.5s\n",
      "[CV 4/5] END .....max_depth=5, n_estimators=100;, score=0.968 total time=   0.5s\n",
      "[CV 5/5] END .....max_depth=5, n_estimators=100;, score=0.951 total time=   0.6s\n",
      "[CV 1/5] END .......max_depth=7, n_estimators=5;, score=0.893 total time=   0.0s\n",
      "[CV 2/5] END .......max_depth=7, n_estimators=5;, score=0.941 total time=   0.0s\n",
      "[CV 3/5] END .......max_depth=7, n_estimators=5;, score=0.923 total time=   0.0s\n",
      "[CV 4/5] END .......max_depth=7, n_estimators=5;, score=0.933 total time=   0.0s\n",
      "[CV 5/5] END .......max_depth=7, n_estimators=5;, score=0.907 total time=   0.0s\n",
      "[CV 1/5] END ......max_depth=7, n_estimators=10;, score=0.919 total time=   0.1s\n",
      "[CV 2/5] END ......max_depth=7, n_estimators=10;, score=0.942 total time=   0.1s\n",
      "[CV 3/5] END ......max_depth=7, n_estimators=10;, score=0.921 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=7, n_estimators=10;, score=0.944 total time=   0.1s\n",
      "[CV 5/5] END ......max_depth=7, n_estimators=10;, score=0.935 total time=   0.1s\n",
      "[CV 1/5] END ......max_depth=7, n_estimators=20;, score=0.950 total time=   0.1s\n",
      "[CV 2/5] END ......max_depth=7, n_estimators=20;, score=0.965 total time=   0.2s\n",
      "[CV 3/5] END ......max_depth=7, n_estimators=20;, score=0.940 total time=   0.1s\n",
      "[CV 4/5] END ......max_depth=7, n_estimators=20;, score=0.969 total time=   0.2s\n",
      "[CV 5/5] END ......max_depth=7, n_estimators=20;, score=0.952 total time=   0.1s\n",
      "[CV 1/5] END ......max_depth=7, n_estimators=30;, score=0.947 total time=   0.2s\n",
      "[CV 2/5] END ......max_depth=7, n_estimators=30;, score=0.957 total time=   0.2s\n",
      "[CV 3/5] END ......max_depth=7, n_estimators=30;, score=0.950 total time=   0.2s\n",
      "[CV 4/5] END ......max_depth=7, n_estimators=30;, score=0.965 total time=   0.2s\n",
      "[CV 5/5] END ......max_depth=7, n_estimators=30;, score=0.951 total time=   0.2s\n",
      "[CV 1/5] END ......max_depth=7, n_estimators=50;, score=0.959 total time=   0.3s\n",
      "[CV 2/5] END ......max_depth=7, n_estimators=50;, score=0.959 total time=   0.3s\n",
      "[CV 3/5] END ......max_depth=7, n_estimators=50;, score=0.961 total time=   0.3s\n",
      "[CV 4/5] END ......max_depth=7, n_estimators=50;, score=0.957 total time=   0.3s\n",
      "[CV 5/5] END ......max_depth=7, n_estimators=50;, score=0.959 total time=   0.3s\n",
      "[CV 1/5] END .....max_depth=7, n_estimators=100;, score=0.956 total time=   0.6s\n",
      "[CV 2/5] END .....max_depth=7, n_estimators=100;, score=0.969 total time=   0.5s\n",
      "[CV 3/5] END .....max_depth=7, n_estimators=100;, score=0.958 total time=   0.6s\n",
      "[CV 4/5] END .....max_depth=7, n_estimators=100;, score=0.969 total time=   0.6s\n",
      "[CV 5/5] END .....max_depth=7, n_estimators=100;, score=0.961 total time=   0.6s\n",
      "[CV 1/5] END ......max_depth=10, n_estimators=5;, score=0.909 total time=   0.0s\n",
      "[CV 2/5] END ......max_depth=10, n_estimators=5;, score=0.922 total time=   0.0s\n",
      "[CV 3/5] END ......max_depth=10, n_estimators=5;, score=0.907 total time=   0.0s\n",
      "[CV 4/5] END ......max_depth=10, n_estimators=5;, score=0.902 total time=   0.0s\n",
      "[CV 5/5] END ......max_depth=10, n_estimators=5;, score=0.939 total time=   0.0s\n",
      "[CV 1/5] END .....max_depth=10, n_estimators=10;, score=0.948 total time=   0.1s\n",
      "[CV 2/5] END .....max_depth=10, n_estimators=10;, score=0.953 total time=   0.1s\n",
      "[CV 3/5] END .....max_depth=10, n_estimators=10;, score=0.914 total time=   0.1s\n",
      "[CV 4/5] END .....max_depth=10, n_estimators=10;, score=0.914 total time=   0.1s\n",
      "[CV 5/5] END .....max_depth=10, n_estimators=10;, score=0.941 total time=   0.1s\n",
      "[CV 1/5] END .....max_depth=10, n_estimators=20;, score=0.946 total time=   0.1s\n",
      "[CV 2/5] END .....max_depth=10, n_estimators=20;, score=0.951 total time=   0.1s\n",
      "[CV 3/5] END .....max_depth=10, n_estimators=20;, score=0.959 total time=   0.1s\n",
      "[CV 4/5] END .....max_depth=10, n_estimators=20;, score=0.957 total time=   0.1s\n",
      "[CV 5/5] END .....max_depth=10, n_estimators=20;, score=0.956 total time=   0.1s\n",
      "[CV 1/5] END .....max_depth=10, n_estimators=30;, score=0.964 total time=   0.2s\n",
      "[CV 2/5] END .....max_depth=10, n_estimators=30;, score=0.967 total time=   0.2s\n",
      "[CV 3/5] END .....max_depth=10, n_estimators=30;, score=0.952 total time=   0.2s\n",
      "[CV 4/5] END .....max_depth=10, n_estimators=30;, score=0.970 total time=   0.2s\n",
      "[CV 5/5] END .....max_depth=10, n_estimators=30;, score=0.961 total time=   0.2s\n",
      "[CV 1/5] END .....max_depth=10, n_estimators=50;, score=0.961 total time=   0.3s\n",
      "[CV 2/5] END .....max_depth=10, n_estimators=50;, score=0.967 total time=   0.3s\n",
      "[CV 3/5] END .....max_depth=10, n_estimators=50;, score=0.965 total time=   0.3s\n",
      "[CV 4/5] END .....max_depth=10, n_estimators=50;, score=0.962 total time=   0.3s\n",
      "[CV 5/5] END .....max_depth=10, n_estimators=50;, score=0.960 total time=   0.3s\n",
      "[CV 1/5] END ....max_depth=10, n_estimators=100;, score=0.964 total time=   0.6s\n",
      "[CV 2/5] END ....max_depth=10, n_estimators=100;, score=0.968 total time=   0.6s\n",
      "[CV 3/5] END ....max_depth=10, n_estimators=100;, score=0.967 total time=   0.6s\n",
      "[CV 4/5] END ....max_depth=10, n_estimators=100;, score=0.965 total time=   0.7s\n",
      "[CV 5/5] END ....max_depth=10, n_estimators=100;, score=0.956 total time=   0.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=<__main__.RandomForestClassifier object at 0x7f9aa76810d0>,\n",
       "             param_grid={'max_depth': [2, 3, 5, 7, 10],\n",
       "                         'n_estimators': [5, 10, 20, 30, 50, 100]},\n",
       "             scoring=make_scorer(roc_auc_score), verbose=3)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'max_depth': 10, 'n_estimators': 100}, c AUC на CV: 0.96\n"
     ]
    }
   ],
   "source": [
    "print(f\"Лучшие параметры: {best_params}, c AUC на CV: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_classifier = RandomForestClassifier(**best_params)\n",
    "val_classifier.fit(X_train_train, y_train_train)\n",
    "y_val_pred = val_classifier.predict(X_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on validation:\n",
      "AUC: 0.96,\n",
      "Accuracy: 0.88,\n",
      "F1:0.82\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Scores on validation:\\nAUC: {roc_auc_score(y_train_val, y_val_pred):.2f},\\nAccuracy: {accuracy_score(y_train_val,(y_val_pred>0.5).astype(int)):.2f},\\nF1:{f1_score(y_train_val,(y_val_pred>0.5).astype(int)):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, _ = roc_curve(y_train_val, y_val_pred, pos_label=1)\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_classifier = RandomForestClassifier(**best_params)\n",
    "final_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обучите модель на всех данных из x_spam_train и y_spam_train.\n",
    "2. Сделайте submit своего решения и получите значение f1_score не менее 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = (final_classifier.predict(X_test) > 0.5).astype(int)\n",
    "df = pd.DataFrame(\n",
    "    {\"Expected\": y_test_preds, \"Id\": np.arange(y_test_preds.shape[0])},\n",
    ")\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest выглядит так:\n",
    "1. Посчитать out-of-bag ошибку предсказания `err_oob` (https://en.wikipedia.org/wiki/Out-of-bag_error)\n",
    "2. Перемешать значения признака `j` у объектов выборки (у каждого из объектов изменится значение признака `j` на какой-то другой)\n",
    "3. Посчитать out-of-bag ошибку (`err_oob_j`) еще раз.\n",
    "4. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_classifier = RandomForestClassifier(**best_params, save_samples_idx=True)\n",
    "final_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oob(rfc, X, y):\n",
    "    oob = []\n",
    "    for tree, features, samples_idx in zip(\n",
    "        rfc._trees,\n",
    "        rfc._features,\n",
    "        rfc._samples_idx,\n",
    "    ):\n",
    "        oob_samples_idx = np.array(\n",
    "            list(set(np.arange(X.shape[0])) - set(samples_idx)), dtype=int\n",
    "        )\n",
    "        oob_X = X[oob_samples_idx][:, features]\n",
    "        oob_y = y[oob_samples_idx]\n",
    "        oob_y_pred = tree.predict(oob_X)\n",
    "        oob_f1 = accuracy_score(oob_y, oob_y_pred)\n",
    "        oob.append(oob_f1)\n",
    "    return np.array(oob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(rfc, X, y):\n",
    "    m, n = X.shape\n",
    "    impor = []\n",
    "    oob_error = compute_oob(rfc, X, y)\n",
    "    for i in range(X.shape[1]):\n",
    "        X_j = np.copy(X)\n",
    "        X_j[:, i] = np.random.choice(X_j[:, i], size=m, replace=False)\n",
    "        oob_error_j = compute_oob(rfc, X_j, y)\n",
    "        impor.append((oob_error - oob_error_j).mean())\n",
    "    return impor\n",
    "\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте решение на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.983\n",
      "Importance: [-0.002088211969101795, 0.002192018062102105, 0.06190600904302892, 0.050579435965379345, 0.1195923745841803, 0.0015455361073504182]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [\n",
    "        (\n",
    "            np.random.randint(0, 2),\n",
    "            np.random.randint(0, 2),\n",
    "            i % 6 == 3,\n",
    "            i % 6 == 0,\n",
    "            i % 3 == 2,\n",
    "            np.random.randint(0, 2),\n",
    "        )\n",
    "        for i in range(size)\n",
    "    ]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100, save_samples_idx=True)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict_nonbinary(X)[0] == y))\n",
    "print(\"Importance:\", feature_importance(rfc, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте, какие признаки важны для датасета spam? (Используйте файлы x_spam_train и y_spam_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9110054347826086\n",
      "Importance: [0.001755475508325648, 0.004649535242677524, 0.007785177165932455, 0.0005508902617626222, 0.011113835739528656, 0.004973873569889704, 0.009408574028220908, 0.0021261822041550803, 0.003784949729887109, 0.007715630911304466, 0.004754894048823713, 0.003992866321290847, 0.0021255643260307223, 0.0006383360614456424, 0.0021832741218030527, 0.014973629814533483, 0.006281655547978017, 0.004833960594130538, 0.009173230936030036, 0.004792810126322862, 0.0162413761535787, 0.0006184476631308633, 0.004952908037770852, 0.010927855819060582, 0.008723726229129121, 0.0052218913440734895, 0.004279456493132803, 0.0018919344474636502, 0.002788622399582723, 0.0031710703249501126, 0.0026351364016855227, 0.0007325762399844626, 0.0009797410082995717, 0.0005506266260826986, 0.0022604708474010414, 0.0011807899411748846, 0.004653295651141968, 1.8121559651843587e-05, 0.001583038420133749, 0.0014386884314764037, 0.00038600371005816705, 0.0014908240771925984, 0.0018858859349507528, 0.0009277661457363517, 0.0023337836414860525, 0.0037608657091566778, 7.33419134377189e-05, 0.0004410992374502087, 0.002206203424595523, 0.0068355194113974005, 0.0009667452375437902, 0.014646091844880048, 0.01736217149287541, 0.0021930195125206497, 0.011845067210819593, 0.025536594907568596, 0.02090237320277493]\n",
      "Most important:  [55 56 52 20 15 51 54  4 23  6 18 24  2  9 49 16 25  5 22 17]\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(**best_params, save_samples_idx=True)\n",
    "rfc.fit(X_train_train, y_train_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict_nonbinary(X_train_train)[0] == y_train_train))\n",
    "impor = feature_importance(rfc, X_train_train, y_train_train)\n",
    "print(\"Importance:\", impor)\n",
    "print(\n",
    "    \"Most important: \",\n",
    "    most_important_features(impor, names=np.arange(X_train_train.shape[1])),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве альтернативы попробуем библиотечные реализации ансамблей моделей. \n",
    "\n",
    "1. [CatBoost](https://catboost.ai/docs/)\n",
    "2. [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n",
    "3. [LightGBM](https://lightgbm.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установите необходимые библиотеки. \n",
    "Возможно, потребуется установка дополнительных пакетов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Примените модели для нашего датасета.\n",
    "\n",
    "2. Для стандартного набора параметров у каждой модели нарисуйте `ROC` кривую и выведите `AUC` и `accuracy`.\n",
    "\n",
    "3. Посчитайте время обучения каждой модели (можно использовать [timeit magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)).\n",
    "\n",
    "4. Сравните метрики качества и скорость обучения моделей. Какие выводы можно сделать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:02:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:02:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:02:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:02:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:02:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:02:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:02:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:02:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1.05 s ± 186 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "xg_classifier = xg.XGBClassifier()\n",
    "xg_classifier.fit(X_train_train, y_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/miniconda3/envs/scientific/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:13:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-----------Xgboost------------\n",
      "Scores on validation:\n",
      "AUC: 0.979,\n",
      "Accuracy: 0.951,\n",
      "F1:0.935\n"
     ]
    }
   ],
   "source": [
    "xg_classifier = xg.XGBClassifier()\n",
    "xg_classifier.fit(X_train_train, y_train_train)\n",
    "y_val_pred_xg = xg_classifier.predict_proba(\n",
    "    X_train_val,\n",
    ")\n",
    "print(\"Xgboost\".center(30, \"-\"))\n",
    "print(\n",
    "    f\"Scores on validation:\\nAUC: {roc_auc_score(y_train_val, y_val_pred_xg[:,1]):.3f},\\nAccuracy: {accuracy_score(y_train_val,y_val_pred_xg.argmax(axis=1)):.3f},\\nF1:{f1_score(y_train_val,y_val_pred_xg.argmax(axis=1)):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.51 s ± 238 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "cb_classifier = CatBoostClassifier(silent=True)\n",
    "cb_classifier.fit(X_train_train,y_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------CatBoost-----------\n",
      "Scores on validation:\n",
      "AUC: 0.980,\n",
      "Accuracy: 0.946,\n",
      "F1:0.927\n"
     ]
    }
   ],
   "source": [
    "cb_classifier = CatBoostClassifier(silent=True)\n",
    "cb_classifier.fit(X_train_train, y_train_train)\n",
    "y_val_pred_cb = cb_classifier.predict_proba(\n",
    "    X_train_val,\n",
    ")\n",
    "print(\"CatBoost\".center(30, \"-\"))\n",
    "print(\n",
    "    f\"Scores on validation:\\nAUC: {roc_auc_score(y_train_val, y_val_pred_cb[:,1]):.3f},\\nAccuracy: {accuracy_score(y_train_val,y_val_pred_cb.argmax(axis=1)):.3f},\\nF1:{f1_score(y_train_val,y_val_pred_cb.argmax(axis=1)):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ваш ответ_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------CatBoost-----------\n",
      "Scores on validation:\n",
      "AUC: 0.981,\n",
      "Accuracy: 0.951,\n",
      "F1:0.935\n"
     ]
    }
   ],
   "source": [
    "lg_classifier = LGBMClassifier()\n",
    "lg_classifier.fit(X_train_train, y_train_train)\n",
    "y_val_pred_lg = lg_classifier.predict_proba(\n",
    "    X_train_val,\n",
    ")\n",
    "print(\"CatBoost\".center(30, \"-\"))\n",
    "print(\n",
    "    f\"Scores on validation:\\nAUC: {roc_auc_score(y_train_val, y_val_pred_lg[:,1]):.3f},\\nAccuracy: {accuracy_score(y_train_val,y_val_pred_lg.argmax(axis=1)):.3f},\\nF1:{f1_score(y_train_val,y_val_pred_lg.argmax(axis=1)):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542 ms ± 141 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lg_classifier = LGBMClassifier()\n",
    "lg_classifier.fit(X_train_train, y_train_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы: библиотечные реализации бустингов с стандартными параметрами дают очень похожее качество и лучше, чем случаный лес(возможно параметры подобраны не лучшие). LGBM быстрее всех, xgboost на втором месте. Однако есть реализации обучения на GPU - там возможен другой порядок. Между этими тремя пакетами я бы выбирал, исходя из из остальных фич: из опыта, catboost хорошо работает с категоральными признакаим."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific",
   "language": "python",
   "name": "scientific"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
